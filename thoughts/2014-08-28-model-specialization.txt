Today as I was using the scanner at our lab and inputing my email address to let the machine send the scanned file to me, I noticed this convenient ".com" key on the keyboard on the touch screen. It is a good example of model specialization. The generic keyboard enables us to input anything we need to input, but it just provides the very basic building blocks for the input task -- single characters. An email address keyboard, being a specialization of the generic keyboard, provides more convenient/larger/assembled/frequently used building blocks such as ".com" and ".edu" to improve the input efficiency. We could imagine a further specialized keyboard with even larger building blocks like "@rpi.edu". I would very much like to use a keyboard like that to tell the scanner my email address and it would improve the efficiency of many people at RPI. So model specialization is about recognizing frequently used composite building blocks and making them as easily available as basic model elements, usually applicable within a certain user group. Models specialized according to the needs of a specific group of people can greatly ease the model application tasks. Applying models to represent knowledge is like using a keyboard to input text. Elements such as classes and properties are just like keys on a keyboard, and using basic elements to define domain knowledge elements is like typing meaningful words with certain keystroke sequences. If we see that a certain keystroke sequence will be typed very frequently, we would naturally hope that there is this single key which inputs the word consisting of the keystroke sequence. Likewise in the modeling area, if we expect a class or property will be defined very frequently, or a certain attribute value will be used very frequently, we would want such a class or property a model built-in and such an attribute value a default.

From the perspective of efforts exerted by people using a certain general model, people using the model for different areas are likely to do very different things with the model, but among those people who are using the model for the same purpose, there are very likely to be some common tasks that every one does. For example, to define a vocabulary service based on the Linked Data API model, every one would want to define an LDA endpoint for listing vocabularies and another one for listing terms in a certain vocabulary. Most of them would want a labelled viewer to show labels instead of URIs of vocabularies and terms, which means they set the default viewer of the LDA instance to be the built-in labelled viewer. If we define a Linked Vocabulary API based on the Linked Data API and in this Linked Vocabluary API the vocabulary listing endpoint and term listing endpoint are pre-defined, as subclasses of the LDA endpoint, and an LVA class subclass of LDA, with labelled viewer as its default viewer, vocabulary service builders will save efforts if they start with the Linked Vocabulary API specifically tailored to meet their needs, compared with starting with the more general Linked Data API. With everyday chores already taken care of by the specialized model, users can better focus on their individual use cases and save some efforts that do not really yield any value.

One question of model specialization is: how many applications there need to be to make a specialized model worthwhile? Overspecialized model, for example, a keyboard containing my email address or a Linked Data API specialization just for a specific vocabulary, with properties of that vocabulary built-in as part of the specialized model, would not be applied in other scenarios and not that useful. Specialization is based on unbalanced usage. People are always or more likely to use the system in one certain way is a good hint that making this specific usage more efficient by re-engineering the system will yield good return on investment. Model specialization is creating shortcuts for usage patterns factored out that will appear many times.

For how many times is a certain usage pattern gonna happen? This question is usually answered with practical experience. A scanner sitting in the RPI campus is of course expecting a lot of users with @rpi.edu email addresses. Publishing vocabularies with Linked Data API is just gaining momentum and becoming more and more popular. Owners of vocabularies want pre-configured Linked Vocabulary API instances to work on their vocabularies. They just want something that is easy to use and not picky about functionality and appearance. In other words, they don't wanna touch most of the configurable parameters, and just wanna copy their OK values from someone else. The need for these OK presets is a strong hint that a model having all these loose ends fixed would be greatly appreciated. Yes, flexibility of a general model becomes loose ends or overwhelming choices for people with a specific intended usage of the model. Without a speicialized model, they will copy specifications defined for other similar usages as the basis of defining their specifications.

How can we evaluate a specialized model? How can we decide if it is under-specialized, proper, or over-specialized? Are there standard process or theory to follow in order to decide in which category our model falls?

Another example of specialization is institutional manuals for the same piece of software. They incorporate institutional context into them to get to really short and coherent and practical guides.

DoCO the Document Components Ontology seems the only part of SPAR I need to know. It is based on Discourse Elements, Structural Patterns, and the SALT Rhetorical Ontology. What do these three mean? What do these three ontologies have?

