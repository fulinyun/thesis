maybe important but not quite capturable or helpful in reproducing the results: the details of the computational environments. No one is likely to replicate the exact same environment. Exposing such environments via Web services would be useful. Portable computational environments such as Python Virtual Environments supported by the tool virtualenv would be really helpful. The readers are not restricted by the functions provided by the Web services created by someone else. They have the full control of their computational environments. Two important credibility tests: exact same environment as the authors used, absolute freedom on what to do in the environment.

so reproducibility does not merely mean readers can reproduce the exact same result as reported in the publication, it rather means that readers get a compatible environment as the authors used to produce the reported results and are free to do whatever they want with that environment, including reproducing exactly the same results as reported in the publication.

http://www.slideshare.net/carolegoble/ismb2013-keynotecleangoble?qid=27c17b78-a9e2-482f-81b3-226bb87360ba&v=qf1&b=&from_search=10 #17: reproduce is same experiment, different set up (what does "same experiment" mean? How same should it be? does parameter tuning make a different experiment?)
related concepts:
repeat: same experiment, same lab
replicate: same experiment different lab
reuse: different experiment some of same
reuse is irrelevant to this thesis
repeat is not the focus -- writer's activity
how are repeat and replicate related? read "chris-drummond-replicability is not reproducibility"
reproducing exactly the same figures, tables etc just checks whether the authors actually carried out the claimed experiments in the paper, which does not benefit the readers much. Allowing the readers to carry out different experiments to validate the scientific work is desirable reproducibility. so reproducibility means how much the published paper with its provenance information benefit the readers in terms of carrying out different experiments from the ones reported in the paper. it is hard to measure since there's no way the authors are to know what experiments the readers would like to carry out. library functions that can be combined in differernt ways provide some degree of reproducibility; documentation on how these library functions can be connected to create data processing workflows help improve reproducibility; scripts defining a specific operation sequence of data processing performed by the authors do not contribute to the reproducibility of the work, except that they can be viewed as examples of data processing workflows consisting of library functions and thus are a kind of documentation.


to enable readers