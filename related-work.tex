%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                           RELATED WORK                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\chapter{RELATED WORK}
\label{related-work}
%\resetfootnote %this command starts footnote numbering with 1 again.

\section{General discussions on provenance and research reproducibility}
\label{sec:reproducibility}
Jarvis in \cite{jarvis2010importance} talks about the importance of provenance in the context of journalism. 
%according to Tan et al \cite{tan2007provenance}, the provenance information discussed in this proposal falls into the category of workflow (or coarse-grained) provenance, where detailed transformation processes of specific pieces of data in the final publications are not captured. For example, the process of generating a table is captured, but not the process that leads to specific columns, rows or cells of the table, which includes data transformation details such as the aggregation function used and the deletion of outliers.

Donoho et al in \cite{donoho2009reproducible} pointed out that current computational science practice, unlike the well-established deductive science and empirical science, doesn't generate routinely verifiable knowledge due to the lack of mature responses to the ubiquity of error in science such as formal logic and mathematical proof for deduction, and statistical hypothesis testing and standardized reproducibility information for empiricism. Following Claerbout's idea that
\begin{quote}
	We publish not just computational results
	but also the complete software environment
	and data that generated those results.
\end{quote}
, Donoho et al developed the Wavelab package which contained a unified set of wavelet and time-frequency tools that reproduce all the calculations in the papers Donoho and his collaborators had written on computational harmonic analysis \cite{buckheit1995wavelab}. In \cite{donoho2009reproducible}, the authors mentioned that reproducible publication packages benefit \emph{strangers} who don't possess our current short-term memory and experiences.
\begin{quote}
	In the heat
	of a computational project, we store many things
	in short-term memory that we need at that moment
	to use the code productively.
\end{quote}
In other words, conducting research in a reproducible manner hurts productivity in the heat of a project, although years from now, we ourselves, not remembering the myriad small details that accumulated in our minds during this project, will become such strangers.

Gentleman and Lang in \cite{gentleman2007statistical} proposed the idea of a \emph{research compendium}. A compendium can include several \emph{dynamic documents} that are mixtures of text and code. They also implemented a prototype by using a modified version of the \texttt{noweb} markup \cite{ramsey1994literate} to write dynamic documents, writing text chunks in a modified version of \LaTeX and writing code in R. Dynamic documents are processed with Sweave \cite{leisch2002sweave} and compendiums are represented as R packages.

Apparently, very few researchers have adopted the communication of research compendiums, as pointed out by Stodden in \cite{stodden2014enabling}. Most, if not all, of the problems listed in the \emph{Quick Answers to Knee-Jerk Objections} section of \cite{donoho2009reproducible} still remain in the way. For example, \emph{reproducibility takes time and effort}, although it may save us time in the long run. \emph{No one else does it, so I won't get any credit for it.} Creating compendiums instead of just papers really look wasting at the first sight. The list of problems goes on and on.

But the cost of not work reproducibly is creeping and hurting really badly despite the fact that it is somewhat hard to work in this way. In her talk in 2011 \cite{stodden2011establishing}, Stodden argued that the lack of transparency is hurting the credibility of scientific research based on her observation in the field of statistics.

In the area of biomedical research, Bustin in \cite{bustin2015reproducibility} pointed out that there is increasing concern about the reliability of biomedical research. It is estimated that up to 85\% of research funding is wasted in unreliable research.

%We plan to look further into the facts and references mentioned in this survey paper after proposal.

%The comment article introduces the series and discusses the consistent and colossal failure of initially promising research findings to translate into improvements in health care because of the many economic, political, social and cultural factors that influence researchers, funders, regulators, institutions and companies \cite{macleod2014biomedical}. 
%The first article in the series pointed out that the research studies supported by around US\$240 billion worldwide investment may have problems at the time funders decide what research to support, causing that much research does not lead to worthwhile achievements and that good research ideas do not yield the anticipated results \cite{chalmers2014increase}. 
%The second article in the series highlights that an absence of detailed written protocols and poor documentation of research is common and that inadequate emphasis is placed on recording of research decisions and on reproducibility of research \cite{ioannidis2014increasing}. 
%The third article discusses the modern approach to the regulation, governance, and management of biomedical research and emphasizes how inefficient management can easily compromise the interests of patients and the public \cite{salman2014increasing}. 
%The fourth article points out that a large percentage of protocols, reports and datasets associated with health research are rarely available, and there is selective reporting of methods and results, which leads to the introduction of bias and wastes huge amounts of research funding \cite{chan2014increasing}. The final article reemphasizes the absolute requirements for accurate, exhaustive and transparent reporting and notes that although reporting guidelines are important, they are all much less adopted and adhered to than they should be \cite{glasziou2014reducing}. The editors of the series make the revolutionary suggestion that rather than using journal impact factors to assess academics, it might be more reasonable to judge the researchers' work with the rigorousness of methodology, the transparency of reporting and the reproducibility of results, which would of course facilitate the publication of more reliable and biologically relevant data.


\section{Provenance capturing approaches}
\cite{groth2009recording} presents five characteristics of provenance information, namely immutable, meaning intact after creation, attributable, meaning clear responsibility, autonomously creatable, meaning created by the most appropriate agent, finalizable, meaning clear timing of completeness, and process reflecting, meaning able to reflect the whole process leading to the final product. It also presents the concept of p-assertions first defined in \cite{groth2006architecture} and the six key actors in provenance-aware systems, namely application, sender, receiver, asserter, recorder and provenance store.

\cite{miles2011prime} presents a provenance question driven methodology for provenance capturing.

Workflow systems such as VisTrails \cite{freire2014reproducibility}, Kepler \cite{ludascher2006scientific}, Taverna \cite{wolstencroft2013taverna} and ReproZip \cite{chirigati2013reprozip} %will be discussed in detail in the final thesis.

\section{Provenance for research publications}
\subsection{Models for publication structure}
\cite{taboada2006rhetorical} covers rhetorical structure theory
\cite{groza2007salt} semantically annotates \LaTeX \ source files so that roles played by each part of the publication are made explicit.
\cite{clark2013micropublications} micropublications: a semantic model for claims, evidence, arguments and annotations in biomedical communications.
%And more to come after proposal...

\subsection{Models for Publication Preparing Process}
PROV-O\footnote{http://www.w3.org/TR/prov-o/} and ProvONE\footnote{http://vcvcomputing.com/provone/provone.html} %will be discussed in the final thesis.

\section{Ontology usability evaluation}
Ontology usability evaluation is an aspect of the more general task of ontology evaluation. As mentioned in \ref{subsec:evaluation}, ontology evaluation starts with checking completeness, consistency and conciseness of ontologies (e.g., \cite{gruninger1995methodology} and \cite{gomez2001evaluation}), so called \emph{deductive approaches} in 

To select an existing ontology to reuse in a new application, \emph{multiple-criteria approaches} (named by Brank et al in \cite{brank2005survey}) were developed. Ontologies are evaluated by several criteria and scores are given per criteria. An overall score of the ontology then could be calculated as the weighted sum of per-criteria scores. For example, Lozano-Tello et al presented the \emph{ONTOMETRIC} method in \cite{lozano2003selection,lozano2004ontometric}, which evaluates ontologies with a taxonomy of
160 characteristics organized in a multilevel framework. At the top level, there are five basic aspects. These are: 
\begin{itemize}
	\item the \emph{content} of the ontology and the
	organization of their contents,
	\item the \emph{language}
	in which it is implemented, 
	\item the \emph{methodology}
	that has been followed to develop it,
	\item the software \emph{tools} used to build and edit
	the ontology, and 
	\item the \emph{costs} that the ontology
	will be necessary in a certain project.
\end{itemize} 
As mentioned by Hartmann et al in \cite{hartmann2005d1}, this method requires a substantial amount of time to specify the relevant criteria and the scores are given quite subjectively.

Burton-Jones et al proposed an objective metric in \cite{burton2005semiotic}, where a set of 10 objective metrics at 4 different levels of a semiotic framework was presented. For example, at the \emph{syntactic} level, \emph{lawfulness} is defined as the total number of breached rules divided by the number of statements in the ontology, and \emph{richness} as the total number of syntactic features used in the ontology divided by the total number of available features in the ontology language. See Table 4 in \cite{burton2005semiotic} for the details of all the metrics. Since all the metrics used are objective, this approach does not require experts' review of the ontologies. However, the evaluation only gives the general quality of the ontology, which may have nothing to do with how well the ontology fits a certain task.

An really interesting work, although still not able to answer the ``how well it fits'' question, is \cite{gangemi2006qood}, where Gangemi et al organized the criteria for ontology evaluation and selection with a semiotic meta-ontology called $O^2$ and the \emph{oQual} evaluation ontology which involves concepts and relations relevant to ontology evaluation and selection. Therefore, evaluation based on the oQual ontology goes beyond the mere calculation of a weighted sum, but also contains reasoning based on the evaluation ontology. For example, the two criteria, namely \emph{explicitness} and \emph{computational efficiency} could be detected as conflicting by this approach because the former goal indicates high rate of cycles in the ontology but the latter indicates low rate of cycles. (See Figure 4 in \cite{gangemi2006qood} for the details.) Such a reasoning mechanism sheds more light than mere scores on the trade-offs to consider for the selection of ontologies.

The few work dealing with ontology usability evaluation includes \cite{casellas2009ontology}, which used System Usability Scale \cite{brooke1996sus} to evaluate the usability of the Ontology of Professional Judicial Knowledge (See Chapter 2 of \cite{casellas2009ontology} for a good introduction of the ontology).

%We also plan to look at the Severity Ratings for Usability Problems\footnote{http://www.nngroup.com/articles/how-to-rate-the-severity-of-usability-problems/ [Retrieved April 12th, 2015]}

%The approach we propose to develop is based on the intuition that better ontologies enable simpler software to accomplish a certain task, so we could evaluate the usability of ontologies by looking at the complexity of necessary software based on them.
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
